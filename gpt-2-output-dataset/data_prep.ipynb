{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First task: My detector code wants three .jsonl files for test, train and val. I have one csv that need to be split accordingly and converted to jsonl. This is the abstract data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "paths_to_convert = [\"/Users/dustinhayes/Desktop/DEEP LEARNING FINAL PROJECT/test.csv\",\n",
    "                    \"/Users/dustinhayes/Desktop/DEEP LEARNING FINAL PROJECT/validation.csv\",\n",
    "                    \"/Users/dustinhayes/Desktop/DEEP LEARNING FINAL PROJECT/train.csv\"]\n",
    "paths_to_gain = [\"detector/TestData/test.jsonl\", \"detector/TestData/validation.jsonl\", \"detector/TestData/train.jsonl\"]\n",
    "\n",
    "for start_path, end_path in zip(paths_to_convert, paths_to_gain):\n",
    "    with open(start_path, mode='r', encoding='utf-8') as csv_file, \\\n",
    "        open(end_path, mode='w', encoding='utf-8') as jsonl_file:\n",
    "\n",
    "        reader = csv.DictReader(csv_file)\n",
    "        for row in reader:\n",
    "            row['label'] = str(1 - int(row['label'].strip()))\n",
    "            row['text'] = row['abstract']\n",
    "            del row['abstract'] # replace 'abstract' key with 'text' to be consistent with other data sources.\n",
    "\n",
    "            cleaned_row = {k: v.strip() for k, v in row.items() if k != 'label'}  # Clean other fields\n",
    "            cleaned_row['label'] = row['label']  # Add the converted label\n",
    "\n",
    "            jsonl_file.write(json.dumps(cleaned_row) + '\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second task: I have downloaded the original GPT-2 output detector data and need to processes it such that the format matched the abtract data.\n",
    "\n",
    "We are coming from: A 'webtext' set of 3 JSONL files (training, text, val) which contains human-written content, and a set of GPT-2 generated 'large-762' files which contain machine written examples. These JSONL files have the fields: ['id', 'text', 'length', 'ended']\n",
    "\n",
    "\n",
    "We need to go to: Three JSONL files, which contain ['text', 'label'], where label is 1 = HUMAN, 0 = ROBOT. I think I would like to keep 'length' hanging around."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_files_to_convert = 'gpt-2-output-dataset/Deep Learning Final Project Data Pile'\n",
    "machine_written_files_to_convert = ['/Users/dustinhayes/Desktop/DEEP LEARNING FINAL PROJECT/gpt-2-output-dataset/Deep Learning Final Project Data Pile/xl-1542M-k40.test.jsonl',\n",
    "                                    '/Users/dustinhayes/Desktop/DEEP LEARNING FINAL PROJECT/gpt-2-output-dataset/Deep Learning Final Project Data Pile/xl-1542M-k40.train.jsonl',\n",
    "                                    '/Users/dustinhayes/Desktop/DEEP LEARNING FINAL PROJECT/gpt-2-output-dataset/Deep Learning Final Project Data Pile/xl-1542M-k40.valid.jsonl']\n",
    "human_written_files_to_convert = ['/Users/dustinhayes/Desktop/DEEP LEARNING FINAL PROJECT/gpt-2-output-dataset/Deep Learning Final Project Data Pile/webtext.test.jsonl',\n",
    "                                  '/Users/dustinhayes/Desktop/DEEP LEARNING FINAL PROJECT/gpt-2-output-dataset/Deep Learning Final Project Data Pile/webtext.train.jsonl',\n",
    "                                  '/Users/dustinhayes/Desktop/DEEP LEARNING FINAL PROJECT/gpt-2-output-dataset/Deep Learning Final Project Data Pile/webtext.valid.jsonl']\n",
    "paths_to_gain = ['/Users/dustinhayes/Desktop/DEEP LEARNING FINAL PROJECT/gpt-2-output-dataset/detector/OriginalGPTDataLarge/test.jsonl',\n",
    "                 '/Users/dustinhayes/Desktop/DEEP LEARNING FINAL PROJECT/gpt-2-output-dataset/detector/OriginalGPTDataLarge/train.jsonl',\n",
    "                 '/Users/dustinhayes/Desktop/DEEP LEARNING FINAL PROJECT/gpt-2-output-dataset/detector/OriginalGPTDataLarge/validation.jsonl']\n",
    "# I am going to attempt to make a very short version to confirm that my GPT-2 detector is getting loaded correctly\n",
    "# Let's say 1000 examples, 500 machine 500 human\n",
    "\n",
    "# Remember, HUMAN = 1, ROBOT = 0\n",
    "RECORDS_TO_FETCH = 1000\n",
    "\n",
    "for machine_file, human_file, new_path in zip(machine_written_files_to_convert,\n",
    "                                                 human_written_files_to_convert,\n",
    "                                                 paths_to_gain):\n",
    "    # machine_file = path_to_files_to_convert + '/' + machine_file\n",
    "    # human_file = path_to_files_to_convert + '/' + human_file\n",
    "    record_list = []\n",
    "    with open(machine_file, mode='r', encoding='utf-8') as machine_f, \\\n",
    "         open(human_file, mode='r', encoding='utf-8') as human_f, \\\n",
    "         open (new_path, mode='w', encoding='utf-8') as output_f:\n",
    "                # Process machine-written records\n",
    "        for i, line in enumerate(machine_f):\n",
    "            if i >= RECORDS_TO_FETCH // 2:\n",
    "                break\n",
    "            record = json.loads(line)\n",
    "            new_record = {\n",
    "                'text': record['text'].replace('\\n\\n', \" \"),\n",
    "                'label': '0',  # ROBOT\n",
    "                'length': record['length']\n",
    "            }\n",
    "            record_list.append(new_record)\n",
    "            # output_f.write(json.dumps(new_record) + '\\n')\n",
    "\n",
    "        # Process human-written records\n",
    "        for i, line in enumerate(human_f):\n",
    "            if i >= RECORDS_TO_FETCH // 2:\n",
    "                break\n",
    "            record = json.loads(line)\n",
    "            new_record = {\n",
    "                'text': record['text'].replace(\"\\n\\n\", \" \"), # Remove new line chars, I am worried they will mess with the paraphraser\n",
    "                'label': '1',  # HUMAN\n",
    "                'length': record['length']\n",
    "            }\n",
    "            record_list.append(new_record)\n",
    "            # output_f.write(json.dumps(new_record) + '\\n')\n",
    "        random.shuffle(record_list)\n",
    "\n",
    "        for record in record_list:\n",
    "            output_f.write(json.dumps(record) + '\\n')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I want a small version of the abstract dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Paths to the original datasets\n",
    "paths_to_convert = [\n",
    "    \"/Users/dustinhayes/Desktop/DEEP LEARNING FINAL PROJECT/test.csv\",\n",
    "    \"/Users/dustinhayes/Desktop/DEEP LEARNING FINAL PROJECT/validation.csv\",\n",
    "    \"/Users/dustinhayes/Desktop/DEEP LEARNING FINAL PROJECT/train.csv\"\n",
    "]\n",
    "\n",
    "# Paths where the smaller datasets will be saved\n",
    "paths_to_gain = [\n",
    "    \"detector/TestData/test.jsonl\",\n",
    "    \"detector/TestData/validation.jsonl\",\n",
    "    \"detector/TestData/train.jsonl\"\n",
    "]\n",
    "\n",
    "# Number of samples per dataset type\n",
    "sample_size = 100\n",
    "\n",
    "for start_path, end_path in zip(paths_to_convert, paths_to_gain):\n",
    "    with open(start_path, mode='r', encoding='utf-8') as csv_file, \\\n",
    "         open(end_path, mode='w', encoding='utf-8') as jsonl_file:\n",
    "\n",
    "        reader = csv.DictReader(csv_file)\n",
    "        count = 0  # Initialize counter for each dataset\n",
    "\n",
    "        for row in reader:\n",
    "            if count >= sample_size:  # Check if the sample size is reached\n",
    "                break  # Stop reading more rows\n",
    "\n",
    "            # Convert label and restructure row\n",
    "            row['label'] = str(1 - int(row['label'].strip()))\n",
    "            row['text'] = row['abstract']\n",
    "            del row['abstract']\n",
    "\n",
    "            # Clean other fields\n",
    "            cleaned_row = {k: v.strip() for k, v in row.items() if k != 'label'}\n",
    "            cleaned_row['label'] = row['label']\n",
    "\n",
    "            # Write to JSONL file\n",
    "            jsonl_file.write(json.dumps(cleaned_row) + '\\n')\n",
    "\n",
    "            count += 1  # Increment counter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "detector_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dustinhayes/Desktop/DEEP LEARNING FINAL PROJECT/venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Vamsi/T5_Paraphrase_Paws\")  \n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"Vamsi/T5_Paraphrase_Paws\").to('cpu')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dustinhayes/Desktop/DEEP LEARNING FINAL PROJECT/venv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/Users/dustinhayes/Desktop/DEEP LEARNING FINAL PROJECT/venv/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:418: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is an example sentence and will be used as an example for Dustin's final presentation in Deep Learning. It demonstrates the capacity of our T5 model to reorder input while maintaining its overall meaning.\n",
      "This is an example of expression, which will be used as an example for Dustin's final presentation on Deep Learning. It shows the capacity of our T5 model to reorder input while maintaining the overall meaning.\n",
      "This is an example phrase that will be used for Dustin's final presentation, and demonstrates the capacity of our T5 model to reorder input while preserving overall meaning.\n",
      "This is an example sentence and will be used as an example for Dustinâ€™s final deep learning presentation, which shows the capacity of our T5 model to reorder input while mantling the overall meaning.\n",
      "This is an example phrase used as an example for Dustin's final presentation on Deep Learning and shows the capacity of our T5 Model to reorder input while maintaining overall meaning.\n"
     ]
    }
   ],
   "source": [
    "sentence = \"This is an example phrase. It will be used as an example for Dustin's Deep learning final presentation. It demonstrates the capacity of our T5 model to reorder input while mantaining overall meaning.\"\n",
    "\n",
    "text =  \"paraphrase: \" + sentence + \" </s>\"\n",
    "\n",
    "encoding = tokenizer.encode_plus(text,pad_to_max_length=True, return_tensors=\"pt\")\n",
    "input_ids, attention_masks = encoding[\"input_ids\"].to(\"cpu\"), encoding[\"attention_mask\"].to(\"cpu\")\n",
    "\n",
    "\n",
    "outputs = model.generate(\n",
    "    input_ids=input_ids, attention_mask=attention_masks,\n",
    "    max_length=256,\n",
    "    do_sample=True,\n",
    "    top_k=80,\n",
    "    top_p=0.95,\n",
    "    early_stopping=True,\n",
    "    num_return_sequences=5\n",
    ")\n",
    "\n",
    "for output in outputs:\n",
    "    line = tokenizer.decode(output, skip_special_tokens=True,clean_up_tokenization_spaces=True)\n",
    "    print(line)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "detector_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

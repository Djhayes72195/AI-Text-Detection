import torch
from torch.utils.data import DataLoader
from transformers import RobertaTokenizer, RobertaForSequenceClassification, RobertaModel
from detector.dataset import *
from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score
import os

# This model did not work very well on 2 outputs generated by GPT4. It is a few years old so...

# Load the tokenizer
TOKENIZER = RobertaTokenizer.from_pretrained('roberta-base')
BATCH_SIZE = 8

BASE_DATA_DIR = 'gpt-2-output-dataset/detector/'
BASE_RESULTS_DIR = 'gpt-2-output-dataset/Results/'
########################################
# Get baseline performance on test set #
########################################
class TestModel:
    def __init__(self, data_dir_name, run_note, model, stop_after=None, before_tuning=True):
        
        if before_tuning:
            self.model_status = 'BEFORE_FINE_TUNING'
        else:
            self.model_status = 'POST_FINE_TUNING'

        self.data_type = data_dir_name
        path_to_data = BASE_DATA_DIR + data_dir_name
        self.corpus = Corpus(data_dir=path_to_data) # change path to data you want
        test_set = self.corpus.test_texts
        test_labels = self.corpus.test_labels


        self.encoded_test = EncodedDataset(test_set, test_labels, TOKENIZER)
        self.test_loader = DataLoader(self.encoded_test, batch_size=BATCH_SIZE, shuffle=True)

        self.predictions = []
        self.true_labels = []
        self.itera = 0
        self.run_note = run_note
        self.model = model

        self.stop_after = stop_after
        self.path_to_results = BASE_RESULTS_DIR + self.model_status + '_' + data_dir_name.lower() + '.json'

    
    def test_model(self):
        self.model.eval()
        for batch in self.test_loader:
            input_ids, attention_mask, label = batch
            self.itera += 1
            with torch.no_grad():
                print("It's alive!")
                outputs = self.model(input_ids, attention_mask=attention_mask)
                logits = outputs.logits
                probabilities = torch.sigmoid(logits)
                print(probabilities)
                # Convert logits to predictions
                batch_predictions = torch.argmax(logits, dim=1)
                self.predictions.extend(batch_predictions.tolist())
                self.true_labels.extend(label.tolist())
                self.calculate_and_record_metrics(record=False)
                if self.stop_after:
                    if self.itera > self.stop_after:
                        break
    
    def calculate_and_record_metrics(self, record=True):
        accuracy = accuracy_score(self.true_labels, self.predictions)
        conf_matrix = confusion_matrix(self.true_labels, self.predictions)
        precision = precision_score(self.true_labels, self.predictions)
        recall = recall_score(self.true_labels, self.predictions)

        print("Accuracy:", accuracy)
        print("Confusion Matrix:\n", conf_matrix)
        print("Precision:", precision)
        print("Recall:", recall)

        results = {
            'Model Status': self.model_status, 
            'data_type': self.data_type,
            'batch_size': BATCH_SIZE,
            'accuracy': accuracy,
            'confusion_matrix': conf_matrix.tolist(),
            'precision': precision,
            'recall': recall,
            'Notes': self.run_note
        }
        if record:
            with open(self.path_to_results, 'w') as file:
                json.dump(results, file, indent=4)

if __name__ == '__main__':
    # Load the checkpoint
    # model_path = '/Users/dustinhayes/Desktop/DEEP LEARNING FINAL PROJECT/gpt-2-output-dataset/detector-base.pt'
    model_path = 'gpt-2-output-dataset/detector/TrainedModels/test_training.pth'
    checkpoint = torch.load(model_path, map_location=torch.device('cpu'))

    model_name = 'roberta-base'
    model = RobertaForSequenceClassification.from_pretrained(model_name)
    if 'model_state_dict' in checkpoint.keys():
        model.load_state_dict(checkpoint['model_state_dict'], strict=False) # There is a slight mismatch in architecture
    # The small mismatch is architecture does not seem to be very impactful. I still get ~90% accuracy on org data.
    else:
        model.load_state_dict(checkpoint, strict=False)
    

    ##############################################################
    # THE CONTENT AFTER THIS POINT SHOULD BE MODIFIED RUN BY RUN #
    ##############################################################
    run_note = """Results from a test run on a small (50 samples) subset of abstract data. The validation accuracy
                was surpisingly high at .91. It was only trained for 2 epochs. I don't understand why it seemed to work, I was just
                testing that the training loop was up and running. 
               """
    test = TestModel(data_dir_name='TestData',
                     run_note=run_note,
                     model=model,
                     stop_after=1000,
                     before_tuning=False)
    test.test_model()
    test.calculate_and_record_metrics()
import torch
from torch.utils.data import DataLoader
from transformers import RobertaTokenizer, RobertaForSequenceClassification, RobertaModel
from detector.dataset import *
from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score

# This model did not work very well on 2 outputs generated by GPT4. It is a few years old so...

# Load the tokenizer
tokenizer = RobertaTokenizer.from_pretrained('roberta-base')

BATCH_SIZE = 8

# Load the checkpoint
model_path = '/Users/dustinhayes/Desktop/DEEP LEARNING FINAL PROJECT/gpt-2-output-dataset/detector-base.pt'
checkpoint = torch.load(model_path, map_location=torch.device('cpu'))


model_name = 'roberta-base'
model = RobertaForSequenceClassification.from_pretrained(model_name)
model.load_state_dict(checkpoint['model_state_dict'], strict=False)
model.eval()  # Set the model to evaluation mode

########################################
# Get baseline performance on test set #
########################################

# test_texts, test_labels = load_texts('Data/test.jsonl')
corpus = Corpus()
test_set = corpus.test_texts
test_labels = corpus.test_labels

# __init__(self, texts: List[str], labels: List[int],
# tokenizer: PreTrainedTokenizer, max_sequence_length: int = None)
encoded_test = EncodedDataset(test_set, test_labels, tokenizer)
test_loader = DataLoader(encoded_test, batch_size=BATCH_SIZE, shuffle=True)
# for test in encoded_test:
#     print('stop and check')




# inputs = tokenizer(
#     "Python and Java are two widely-used programming languages, each with its own unique characteristics. Python is known for its clean and concise syntax, which emphasizes readability and reduces the need for excessive code. It employs indentation to define code blocks, making it quite approachable for beginners. In contrast, Java's syntax is more verbose, requiring semicolons and curly braces for code structure, which can make it appear less concise compared to Python. Another fundamental difference lies in their type systems. Python is dynamically typed, meaning that variable types are determined at runtime, offering flexibility but also potentially leading to runtime errors. Java, on the other hand, is statically typed, requiring explicit type declarations for variables. This static typing allows for catching type-related errors at compile-time, which can enhance code reliability.",
#     padding=True, 
#     truncation=True, 
#     max_length=512, 
#     return_tensors="pt"
# )
print('examine')

predictions = []
true_labels = []
itera = 0

for batch in test_loader:
    input_ids, attention_mask, label = batch
    itera += 1
    with torch.no_grad():
        print('I got here at least')
        outputs = model(input_ids, attention_mask=attention_mask)
        logits = outputs.logits
        probabilities = torch.sigmoid(logits)
        # Convert logits to predictions
        batch_predictions = torch.argmax(logits, dim=1)
        predictions.extend(batch_predictions.tolist())
        true_labels.extend(label.tolist())
        if itera > 5:
            break


accuracy = accuracy_score(true_labels, predictions)
conf_matrix = confusion_matrix(true_labels, predictions)
precision = precision_score(true_labels, predictions)
recall = recall_score(true_labels, predictions)

print("Accuracy:", accuracy)
print("Confusion Matrix:\n", conf_matrix)
print("Precision:", precision)
print("Recall:", recall)


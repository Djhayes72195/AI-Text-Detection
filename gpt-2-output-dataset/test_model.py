import torch
from torch.utils.data import DataLoader
from transformers import RobertaTokenizer, RobertaForSequenceClassification, RobertaModel
from detector.dataset import *
from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score

# This model did not work very well on 2 outputs generated by GPT4. It is a few years old so...

# Load the tokenizer
tokenizer = RobertaTokenizer.from_pretrained('roberta-base')

BATCH_SIZE = 8

# Load the checkpoint
model_path = '/Users/dustinhayes/Desktop/DEEP LEARNING FINAL PROJECT/gpt-2-output-dataset/detector-base.pt'
checkpoint = torch.load(model_path, map_location=torch.device('cpu'))


model_name = 'roberta-base'
model = RobertaForSequenceClassification.from_pretrained(model_name)
model.load_state_dict(checkpoint['model_state_dict'], strict=False)
model.eval()  # Set the model to evaluation mode

########################################
# Get baseline performance on test set #
########################################

corpus = Corpus(data_dir='gpt-2-output-dataset/detector/ParaphrasedDataFreq=whole') # change path to data you want
test_set = corpus.test_texts
test_labels = corpus.test_labels


encoded_test = EncodedDataset(test_set, test_labels, tokenizer)
test_loader = DataLoader(encoded_test, batch_size=BATCH_SIZE, shuffle=True)


predictions = []
true_labels = []
itera = 0

for batch in test_loader:
    input_ids, attention_mask, label = batch
    itera += 1
    with torch.no_grad():
        print('I got here at least')
        outputs = model(input_ids, attention_mask=attention_mask)
        logits = outputs.logits
        probabilities = torch.sigmoid(logits)
        print(probabilities)
        # Convert logits to predictions
        batch_predictions = torch.argmax(logits, dim=1)
        predictions.extend(batch_predictions.tolist())
        true_labels.extend(label.tolist())
        if itera > 5:
            break

accuracy = accuracy_score(true_labels, predictions)
conf_matrix = confusion_matrix(true_labels, predictions)
precision = precision_score(true_labels, predictions)
recall = recall_score(true_labels, predictions)

print("Accuracy:", accuracy)
print("Confusion Matrix:\n", conf_matrix)
print("Precision:", precision)
print("Recall:", recall)
